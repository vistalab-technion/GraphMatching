{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sanketh/DANI/GraphMatching/\")\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from subgraph_matching_via_nn.data.data_loaders import load_graph\n",
    "from powerful_gnns.util import S2VGraph\n",
    "from powerful_gnns.util import load_data_given_graph_list_and_label_map\n",
    "from powerful_gnns.models.graphcnn import GraphCNN\n",
    "from powerful_gnns.util import separate_data\n",
    "from powerful_gnns.classifier_training import train, test\n",
    "from common.EmbeddingCalculationsService import pairwise_l2_distance, show_distance_matrix, \\\n",
    "    calculate_energy_based_hidden_rep\n",
    "from subgraph_matching_via_nn.training.PairSampleInfo import Pair_Sample_Info\n",
    "from subgraph_matching_via_nn.data.annotated_graph import AnnotatedGraph\n",
    "from subgraph_matching_via_nn.graph_metric_networks.graph_metric_nn import MLPGraphMetricNetwork\n",
    "from subgraph_matching_via_nn.graph_metric_networks.embedding_metric_nn import EmbeddingMetricNetwork\n",
    "from subgraph_matching_via_nn.graph_embedding_networks.gnn_embedding_network import GNNEmbeddingNetwork\n",
    "from subgraph_matching_via_nn.training.trainer.S2VGraphEmbeddingSimilarityMetricTrainer import \\\n",
    "    S2VGraphEmbeddingSimilarityMetricTrainer\n",
    "from subgraph_matching_via_nn.graph_metric_networks.graph_metric_nn import SingleEmbeddingGraphMetricNetwork\n",
    "from subgraph_matching_via_nn.graph_metric_networks.graph_metric_nn import S2VGraphEmbeddingGraphMetricNetwork\n",
    "from networkx import NetworkXError\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import contextlib\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from os import cpu_count\n",
    "import pickle\n",
    "from common.graph_utils import GED_graph_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_s2v_graphs(networkx_graphs, print_stats=True):\n",
    "    g_list = []\n",
    "    label_dict = {}\n",
    "    for networkx_graph in networkx_graphs:\n",
    "        g_list.append(S2VGraph(networkx_graph, label=None))\n",
    "\n",
    "        if not label in label_dict:\n",
    "            mapped = len(label_dict)\n",
    "            label_dict[label] = mapped\n",
    "\n",
    "    graphs, _ = load_data_given_graph_list_and_label_map(g_list, label_dict, degree_as_tag=True, device=device, print_stats=print_stats)\n",
    "    for s2v_graph in graphs:\n",
    "        # convert graph features here, not only in trainer! (heatmap is probably wrong)\n",
    "        annotated_graph = AnnotatedGraph(s2v_graph.g)\n",
    "        s2v_graph.node_features = annotated_graph.node_indicator.to(device=device)\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "goal_subcircuit_name = 'alu'\n",
    "DATA_PATH = 'C:/Users/kogan/OneDrive/Desktop/Research/AMIT/GraphMatching/subgraph_matching_via_nn/data/subcircuits/'\n",
    "DATA_PATH = '../subgraph_matching_via_nn/data/subcircuits/'\n",
    "\n",
    "desktop = pathlib.Path(DATA_PATH)\n",
    "subgraphs = []\n",
    "labels = []\n",
    "for circuit_dir in desktop.iterdir():\n",
    "    if circuit_dir.is_dir():\n",
    "        for subcircuit_file in circuit_dir.iterdir():\n",
    "            if subcircuit_file.is_file():\n",
    "                file_name = subcircuit_file.name\n",
    "                if file_name == 'full_graph.p':\n",
    "                    file_rel_path = f\"{os.sep}{file_name}\"\n",
    "                    loader_params = {\n",
    "                     'data_path' : str(circuit_dir),\n",
    "                     'g_full_path': file_rel_path,\n",
    "                     'g_sub_path': file_rel_path}\n",
    "\n",
    "                    sub_graph = \\\n",
    "                        load_graph(type='subcircuit',\n",
    "                                   loader_params=loader_params)\n",
    "\n",
    "                    if goal_subcircuit_name in circuit_dir.name:\n",
    "                        labels.append(1)\n",
    "                    else:\n",
    "                        # continue #TODO?\n",
    "                        labels.append(0)\n",
    "                    subgraphs.append(sub_graph)\n",
    "\n",
    "labels = np.array(labels, dtype='float32')\n",
    "N = len(subgraphs)\n",
    "N_training = int(2 / 3 * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "subgraphs = []\n",
    "labels = []\n",
    "\n",
    "circuit_base_dir = 'C:\\\\Users\\\\kogan\\\\OneDrive\\\\Desktop\\\\Research\\\\AMIT\\\\GraphMatching\\\\subgraph_matching_via_nn\\\\data\\\\subcircuits\\\\'\n",
    "circuit_base_dir = '/home/sanketh/DANI/GraphMatching/subgraph_matching_via_nn/data/subcircuits/'\n",
    "\n",
    "\n",
    "for circuit_file_name in ['adder_4', 'alu_4', 'alu_8', 'alu_16', 'alu_32', 'mul_4_4', 'mul_4_8', 'mul_8_8', 'mul_16_16', 'mul_16_32']:\n",
    "    file_rel_path = 'full_graph.p'\n",
    "    circuit_dir = f\"{circuit_base_dir}{circuit_file_name}{os.sep}\"\n",
    "    loader_params = {\n",
    "     'data_path' : str(circuit_dir),\n",
    "     'g_full_path': file_rel_path,\n",
    "     'g_sub_path': file_rel_path}\n",
    "\n",
    "    sub_graph = \\\n",
    "        load_graph(type='subcircuit',\n",
    "                   loader_params=loader_params)\n",
    "    subgraphs.append(sub_graph)\n",
    "    labels.append(0)\n",
    "labels[0] = 1\n",
    "\n",
    "N = len(subgraphs)\n",
    "N_training = int(2 / 3 * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Transform graphs into S2VGraph-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g_list = []\n",
    "label_dict = {}\n",
    "for subgraph, label in zip(subgraphs, labels):\n",
    "    g_list.append(S2VGraph(subgraph.G, label))\n",
    "\n",
    "    if not label in label_dict:\n",
    "        mapped = len(label_dict)\n",
    "        label_dict[label] = mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Process graph features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "graphs, num_classes = load_data_given_graph_list_and_label_map(g_list, label_dict, degree_as_tag=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Setup model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#set up seeds and gpu device\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "##k-fold cross validation. Conduct an experiment on the fold specified by args.fold_idx.\n",
    "# train_graphs, test_graphs = separate_data(graphs, seed=0, fold_idx=0, n_splits=1)\n",
    "# train_graphs = graphs[:4]\n",
    "# test_graphs = graphs[4:]\n",
    "train_graphs = test_graphs = graphs\n",
    "\n",
    "model = GraphCNN(num_layers=5, num_mlp_layers = 2, input_dim=train_graphs[0].node_features.shape[1], hidden_dim=64, output_dim=num_classes, final_dropout=0.5, learn_eps=False, graph_pooling_type=\"sum\", neighbor_pooling_type=\"sum\", device=device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "liveloss = PlotLosses(mode='notebook')\n",
    "\n",
    "filename = \"\"\n",
    "epochs = 3\n",
    "k_update_plot = 25\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = train(iters_per_epoch=50, batch_size=len(train_graphs), model=model, device=device, train_graphs=train_graphs, optimizer=optimizer, epoch=epoch)\n",
    "    acc_train, acc_test = test(model, device, train_graphs, test_graphs, epoch)\n",
    "\n",
    "    if not filename == \"\":\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"%f %f %f\" % (avg_loss, acc_train, acc_test))\n",
    "            f.write(\"\\n\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(model.eps)\n",
    "\n",
    "    if epoch % k_update_plot == 0:\n",
    "        liveloss.update({'train error': avg_loss.item()})\n",
    "        liveloss.send()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Show predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(train_graphs).max(1, keepdim=True)[1]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = model(test_graphs).max(1, keepdim=True)[1]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "show all distances matrix (margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_embeddings = model.get_embedding(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l2_dists = pairwise_l2_distance(all_embeddings)\n",
    "\n",
    "rounding_constant = 10 ** 3\n",
    "cos_dists = torch.round(calculate_energy_based_hidden_rep(all_embeddings, threshold=-1) * rounding_constant) / rounding_constant\n",
    "\n",
    "show_distance_matrix(l2_dists, \"l2-distances\")\n",
    "show_distance_matrix(cos_dists, \"cosine-distances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "what is the smallest margin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_min_non_diagonal_entry(tensor_):\n",
    "    tensor_with_inf_diag = tensor_ + torch.diag_embed(torch.ones(tensor_.shape[0], device=device) * float(\"inf\"))\n",
    "    return torch.min(tensor_with_inf_diag).item()\n",
    "\n",
    "print(get_min_non_diagonal_entry(l2_dists))\n",
    "print(get_min_non_diagonal_entry(cos_dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Contrastive loss training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Assemble graph pairs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples_list = []\n",
    "reference_subgraph_index = 0\n",
    "reference_subgraph = subgraphs[reference_subgraph_index]\n",
    "negative_examples_subgraphs = subgraphs[1:]\n",
    "\n",
    "for subgraph in subgraphs:\n",
    "    if subgraph == reference_subgraph:\n",
    "        is_negative_example = False\n",
    "    else:\n",
    "        is_negative_example = True\n",
    "\n",
    "    G1_annotated = AnnotatedGraph(subgraph.G)\n",
    "    G2_annotated = AnnotatedGraph(reference_subgraph.G)\n",
    "    train_samples_list.append(Pair_Sample_Info(\n",
    "        subgraph=G1_annotated,\n",
    "        masked_graph=G2_annotated,\n",
    "        is_negative_sample=torch.tensor(is_negative_example)))\n",
    "\n",
    "val_samples_list = train_samples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# input dim matches the node mask (w) shape\n",
    "input_dim = S2VGraphEmbeddingSimilarityMetricTrainer.get_model_expected_input_dim(train_samples_list[0])\n",
    "model_factory_func = lambda device: GraphCNN(num_layers=5, num_mlp_layers = 2, input_dim=input_dim, hidden_dim=64, output_dim=num_classes, final_dropout=0.5, learn_eps=False, graph_pooling_type=\"sum\", neighbor_pooling_type=\"sum\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define graph metric to train, and the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solver_params = {\n",
    "    \"device\": device,\n",
    "    \"lr\": 1e-3, \"weight_decay\": 1e-3,\n",
    "    \"max_epochs\": 50,\n",
    "    \"cycle_patience\": 5, \"step_size_up\": 10, \"step_size_down\": 10,\n",
    "    \"loss_convergence_threshold\": None,\n",
    "    \"train_loss_convergence_threshold\": 1e-3,\n",
    "    \"successive_convergence_min_iterations_amount\": 5,\n",
    "    \"margin_loss_margin_value\": 5,\n",
    "    \"max_grad_norm\": 0.1,\n",
    "    \"k_update_plot\": 1,\n",
    "    \"train_monitoring_epochs_pace\": 5,\n",
    "    \"batch_size\": len(subgraphs),\n",
    "    \"num_workers\": 0,\n",
    "    \"is_use_model_compliation\": False, #Only supported for Pytorch > 2.0\n",
    "}\n",
    "\n",
    "problem_params = {\"input_dim\": input_dim}\n",
    "\n",
    "dump_base_path = f\".{os.sep}runlogs\"\n",
    "\n",
    "def init_embedding_net_and_trainer(model_factory_func):\n",
    "\n",
    "    # must start on CPU to allow moving model to GPU, due to existing pytorch bug\n",
    "    device = 'cpu'\n",
    "    model = model_factory_func(device=device)\n",
    "\n",
    "    if solver_params['is_use_model_compliation']:\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "    embedding_metric_network = EmbeddingMetricNetwork(loss_fun=loss_fun)\n",
    "\n",
    "    gnn_embedding_nn = GNNEmbeddingNetwork(gnn_model=model)\n",
    "\n",
    "    # embedding_nns = \\\n",
    "    #     [\n",
    "    #         GNNEmbeddingNetwork(gnn_model=model),\n",
    "    #     ]\n",
    "\n",
    "    graph_metric_nn = S2VGraphEmbeddingGraphMetricNetwork(embedding_network=gnn_embedding_nn,\n",
    "                                           embdding_metric_network=embedding_metric_network,\n",
    "                                           device=device)\n",
    "\n",
    "    # graph_metric_nn = SingleEmbeddingGraphMetricNetwork(embedding_network=embedding_nns[0],\n",
    "    #                                        embdding_metric_network=embedding_metric_network,\n",
    "    #                                        device=device)\n",
    "\n",
    "    # graph_metric_nn = MLPGraphMetricNetwork(embedding_networks=embedding_nns,\n",
    "    #                                         embdding_metric_network=embedding_metric_network,\n",
    "    #                                         device=device)\n",
    "\n",
    "\n",
    "    trainer = S2VGraphEmbeddingSimilarityMetricTrainer(graph_metric_nn, dump_base_path,\n",
    "                                      problem_params, solver_params)\n",
    "\n",
    "    return trainer, graph_metric_nn, model\n",
    "\n",
    "trainer, graph_metric_nn, model = init_embedding_net_and_trainer(model_factory_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_distances_heatmap(graphs, model):\n",
    "\n",
    "    model.eval()\n",
    "    all_embeddings = model.get_embedding(graphs)\n",
    "\n",
    "    l2_dists = pairwise_l2_distance(all_embeddings)\n",
    "\n",
    "    rounding_constant = 10 ** 3\n",
    "    cos_dists = torch.round(calculate_energy_based_hidden_rep(all_embeddings, threshold=-1) * rounding_constant) / rounding_constant\n",
    "\n",
    "    show_distance_matrix(l2_dists, \"l2-distances\")\n",
    "    show_distance_matrix(cos_dists, \"cosine-distances\")\n",
    "    print(f\"Off matrix diagonal margin: {get_min_non_diagonal_entry(l2_dists)}\")\n",
    "    print(f\"Off matrix diagonal margin: {get_min_non_diagonal_entry(cos_dists)}\")\n",
    "\n",
    "show_distances_heatmap(generate_s2v_graphs([graph.g for graph in graphs]), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(train_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train(processes_device_ids=[0], use_existing_data_loaders=False, train_samples_list=train_samples_list, val_samples_list=val_samples_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Add ged=1 examples as positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_perturbed_graphs(reference_graph: nx.Graph):\n",
    "    # assumption: there are no self loops, and no duplicate edges of opposite directions\n",
    "\n",
    "    # graphs generated by removing an existing edge\n",
    "    for edge in reference_graph.edges:\n",
    "        copy_graph = reference_graph.copy()\n",
    "        copy_graph.remove_edge(*edge)\n",
    "        yield copy_graph\n",
    "\n",
    "    # graphs generated by adding a missing edge\n",
    "    sorted_nodes = sorted(reference_graph.nodes, reverse=False)\n",
    "    for i, node_i in enumerate(sorted_nodes):\n",
    "        if i == len(sorted_nodes) - 1:\n",
    "            break\n",
    "\n",
    "        for node_j in sorted_nodes[i+1:]:\n",
    "            edge = (node_i, node_j)\n",
    "            if reference_graph.has_edge(*edge):\n",
    "                continue\n",
    "            copy_graph = reference_graph.copy()\n",
    "            copy_graph.add_edge(*edge)\n",
    "            yield copy_graph\n",
    "\n",
    "# generate subgraphs for which ged(subgraph, reference_subgraph)==1\n",
    "perturbed_graphs = list(generate_perturbed_graphs(reference_subgraph.G_sub))\n",
    "for perturbed_graph in perturbed_graphs:\n",
    "    is_negative_example = False\n",
    "\n",
    "    G1_annotated = AnnotatedGraph(perturbed_graph)\n",
    "    G2_annotated = AnnotatedGraph(reference_subgraph.G)\n",
    "    train_samples_list.append(Pair_Sample_Info(\n",
    "        subgraph=G1_annotated,\n",
    "        masked_graph=G2_annotated,\n",
    "        is_negative_sample=torch.tensor(is_negative_example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(train_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_samples_list = train_samples_list\n",
    "model_factory_func = lambda device: GraphCNN(num_layers=5, num_mlp_layers = 2, input_dim=input_dim, hidden_dim=64, output_dim=num_classes, final_dropout=0.5, learn_eps=False, graph_pooling_type=\"sum\", neighbor_pooling_type=\"sum\", device=device)\n",
    "solver_params[\"batch_size\"] = len(train_samples_list)\n",
    "\n",
    "trainer, graph_metric_nn, model = init_embedding_net_and_trainer(model_factory_func)\n",
    "\n",
    "trainer.train(processes_device_ids=[0], use_existing_data_loaders=False, train_samples_list=train_samples_list, val_samples_list=val_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_s2v_graphs = generate_s2v_graphs([subgraph.G for subgraph in subgraphs] + perturbed_graphs)\n",
    "show_distances_heatmap(all_s2v_graphs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show_distances_heatmap(all_s2v_graphs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show_distances_heatmap(all_s2v_graphs[:len(subgraphs)], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reference_s2v_graph = all_s2v_graphs[reference_subgraph_index]\n",
    "reference_graph_with_positive_examples = [reference_s2v_graph] + all_s2v_graphs[len(subgraphs):]\n",
    "reference_graph_with_negative_examples = [all_s2v_graphs[i] for i in range(len(all_s2v_graphs[:len(subgraphs)])) if i != reference_subgraph_index]\n",
    "\n",
    "show_distances_heatmap(reference_graph_with_positive_examples, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_embeddings = model.get_embedding(reference_graph_with_positive_examples)\n",
    "l2_dists = pairwise_l2_distance(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "positive_distances = []\n",
    "for positive_example_second_graph in reference_graph_with_positive_examples:\n",
    "    distance = graph_metric_nn.forward([(reference_s2v_graph, positive_example_second_graph)]).item()\n",
    "    positive_distances.append(distance)\n",
    "    if distance > solver_params['margin_loss_margin_value']:\n",
    "        print(f\"positive loss term not zero, as distance for this positive example pair is {distance}\")\n",
    "\n",
    "negative_distances = []\n",
    "for negative_example_second_graph in reference_graph_with_negative_examples:\n",
    "    distance = graph_metric_nn.forward([(reference_s2v_graph, negative_example_second_graph)]).item()\n",
    "    negative_distances.append(distance)\n",
    "    if distance < solver_params['margin_loss_margin_value']:\n",
    "        print(f\"negative loss term not zero, as distance for this negative example pair is {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max(positive_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min(negative_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "k-subgraphs experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pair_example(G1_annotated, G2_annotated, is_negative_example):\n",
    "    return Pair_Sample_Info(\n",
    "        subgraph=G1_annotated,\n",
    "        masked_graph=G2_annotated,\n",
    "        is_negative_sample=torch.tensor(is_negative_example))\n",
    "\n",
    "def compare_graphs_and_generate_pair_example(G1_annotated, G2_annotated):\n",
    "    SG1 = G1_annotated.g\n",
    "    SG2 = G2_annotated.g\n",
    "\n",
    "    if nx.is_isomorphic(SG1, SG2):\n",
    "        isomorphic_pairs.append((SG1, SG2))\n",
    "        return generate_pair_example(G1_annotated, G2_annotated, is_negative_example = False)\n",
    "    else:\n",
    "        return generate_pair_example(G1_annotated, G2_annotated, is_negative_example = True)\n",
    "    \n",
    "    # try:\n",
    "    #     diff_graph = nx.symmetric_difference(SG1, SG2)\n",
    "    # except NetworkXError:\n",
    "    #     # node sets are different\n",
    "    #     if nx.is_empty(SG1) and nx.is_empty(SG2) and (len(SG1) == len(SG2)):\n",
    "    #         # positive example\n",
    "    #         print(\"node sets are different, but isomorphic graphs\")\n",
    "    #         return generate_pair_example(G1_annotated, G2_annotated, is_negative_example = False)\n",
    "    #     else:\n",
    "    #         # negative example\n",
    "    #         print(\"node sets are different, and not isomorphic graphs\")\n",
    "    #         return generate_pair_example(G1_annotated, G2_annotated, is_negative_example = True)\n",
    "\n",
    "    # if nx.is_empty(diff_graph):\n",
    "    #     # print(\"isomorphic graphs\")\n",
    "    #     isomorphic_pairs.append((SG1, SG2))\n",
    "    #     # positive example\n",
    "    #     return generate_pair_example(G1_annotated, G2_annotated, is_negative_example = False)\n",
    "    # else:\n",
    "    #     # negative example\n",
    "    #     return generate_pair_example(G1_annotated, G2_annotated, is_negative_example = True)\n",
    "\n",
    "K = 11\n",
    "full_graph = reference_subgraph.G\n",
    "\n",
    "subgraphs_iterator = itertools.combinations(full_graph, K)\n",
    "k_subgraphs = [full_graph.subgraph(s) for s in subgraphs_iterator]\n",
    "k_subgraph_annotated_graphs = [AnnotatedGraph(g) for g in k_subgraphs]\n",
    "\n",
    "train_samples_list = []\n",
    "isomorphic_pairs = []\n",
    "train_sample_indices_tuple_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for subgraph_counter1, SG_annotated_1 in enumerate(k_subgraph_annotated_graphs):\n",
    "    for subgraph_counter2 in range(subgraph_counter1+1, len(k_subgraph_annotated_graphs)):\n",
    "        SG_annotated_2 = k_subgraph_annotated_graphs[subgraph_counter2]\n",
    "        train_samples_list.append(compare_graphs_and_generate_pair_example(SG_annotated_1, SG_annotated_2))\n",
    "        train_sample_indices_tuple_list.append((subgraph_counter1, subgraph_counter2))\n",
    "        # if train_samples_list[-1].is_negative_sample == False:\n",
    "        #     print(f\"{subgraph_counter1} vs {subgraph_counter2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ged_pairs_dump_base_path = f\".{os.sep}ged_pairs\n",
    "\n",
    "# loaded_ged_examples = []\n",
    "# for i, doc_path in enumerate(pathlib.Path(ged_pairs_dump_base_path).rglob(\"*.p*\")):\n",
    "#     k_subgraph_ged_pairs_list = pickle.load(open(doc_path, 'rb'))\n",
    "#     loaded_ged_examples += k_subgraph_ged_pairs_list\n",
    "#     print(i)\n",
    "\n",
    "# if not os.path.exists(ged_pairs_dump_base_path):\n",
    "#     os.makedirs(ged_pairs_dump_base_path)\n",
    "#\n",
    "# # k_subgraph_to_perturbed_graphs_map = {}\n",
    "# def generate_ged_paris(subgraph_counter1):\n",
    "#     train_samples_list = []\n",
    "#     SG1 = k_subgraph_annotated_graphs[subgraph_counter1]\n",
    "#     ged1_perturbed_graphs = list(generate_perturbed_graphs(SG1.g))\n",
    "#     for ged1_perturbed_graph in ged1_perturbed_graphs:\n",
    "#         # positive example\n",
    "#         train_samples_list.append(generate_pair_example(SG1, AnnotatedGraph(ged1_perturbed_graph), is_negative_example = False))\n",
    "#\n",
    "#         ged2_perturbed_graphs = list(generate_perturbed_graphs(ged1_perturbed_graph))\n",
    "#         for ged2_perturbed_graph in ged2_perturbed_graphs:\n",
    "#             # could be positive or negative example\n",
    "#             train_samples_list.append(compare_graphs_and_generate_pair_example(k_subgraph_annotated_graphs[subgraph_counter1], AnnotatedGraph(ged2_perturbed_graph)))\n",
    "#\n",
    "#     path = os.path.join(ged_pairs_dump_base_path, f\"subgraph_{subgraph_counter1}.p\")\n",
    "#     with open(path, 'wb') as f:\n",
    "#         pickle.dump(train_samples_list, f)\n",
    "#     return train_samples_list\n",
    "#\n",
    "# total_tasks = len(k_subgraphs)\n",
    "# with tqdm_joblib(tqdm(desc=\"My calculation\", total=total_tasks)) as progress_bar:\n",
    "#     #Parallel(n_jobs=16)(delayed(long_task)() for i in range(10))\n",
    "#\n",
    "#     ged_examples = Parallel(n_jobs=int(cpu_count()), prefer='processes')(\n",
    "#         delayed(generate_ged_paris)(subgraph_counter1=subgraph_counter1)\n",
    "#         for subgraph_counter1 in range(total_tasks)\n",
    "#     )\n",
    "#\n",
    "# # generate subgraphs for which GED==1\n",
    "# for subgraph_counter1, SG1 in enumerate(k_subgraphs):\n",
    "#     print(subgraph_counter1)\n",
    "#     ged1_perturbed_graphs = list(generate_perturbed_graphs(SG1))\n",
    "#     for ged1_perturbed_graph in ged1_perturbed_graphs:\n",
    "#         # positive example\n",
    "#         train_samples_list.append(generate_pair_example(SG1, ged1_perturbed_graph, is_negative_example = False))\n",
    "#         #\n",
    "#         # ged2_perturbed_graphs = list(generate_perturbed_graphs(ged1_perturbed_graph))\n",
    "#         # for ged2_perturbed_graph in ged2_perturbed_graphs:\n",
    "#         #     # could be positive or negative example\n",
    "#         #     train_samples_list.append(compare_graphs_and_generate_pair_example(k_subgraph_annotated_graphs[subgraph_counter1], AnnotatedGraph(ged2_perturbed_graph)))\n",
    "#     # k_subgraph_to_perturbed_graphs_map[SG1] = perturbed_graphs\n",
    "\n",
    "#\n",
    "#\n",
    "# # generate all pairs examples\n",
    "# for SG1, SG1_perturbed_graphs in k_subgraph_to_perturbed_graphs_map.items():\n",
    "#     for SG2, SG2_perturbed_graphs in k_subgraph_to_perturbed_graphs_map.items():\n",
    "#\n",
    "#         if SG1 == SG2:\n",
    "#             continue # trivial pairs, or pairs already considered\n",
    "#\n",
    "#         for SG2_perturbed_graph in SG2_perturbed_graphs:\n",
    "#             train_samples_list.append(compare_graphs_and_generate_pair_example(SG1, SG2_perturbed_graph))\n",
    "#\n",
    "#         for SG1_perturbed_graph in SG1_perturbed_graphs:\n",
    "#             train_samples_list.append(compare_graphs_and_generate_pair_example(SG1_perturbed_graph, SG2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_random_ged_paris(annotated_subgraph: AnnotatedGraph, ged_dist: int, pairs_n: int, is_negative_example: bool):\n",
    "    train_samples_list = []\n",
    "    SG1 = annotated_subgraph\n",
    "\n",
    "    ged_graph_generator = GED_graph_generator(SG1.g, ged_dist)\n",
    "\n",
    "    # generate pairs\n",
    "\n",
    "    for i, ged_perturbed_graph in enumerate(ged_graph_generator.generate()):\n",
    "        if i == pairs_n:\n",
    "            break\n",
    "\n",
    "        # positive example\n",
    "        train_samples_list.append(generate_pair_example(SG1, AnnotatedGraph(ged_perturbed_graph), is_negative_example = is_negative_example))\n",
    "\n",
    "    return train_samples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_k_subgraphs = k_subgraph_annotated_graphs.copy()\n",
    "random.shuffle(random_k_subgraphs)\n",
    "random_k_subgraphs = random_k_subgraphs#[:10]\n",
    "\n",
    "pairs_n = -1\n",
    "\n",
    "total_tasks = len(random_k_subgraphs)\n",
    "with tqdm_joblib(tqdm(desc=\"My calculation\", total=total_tasks)) as progress_bar:\n",
    "    ged_2_examples = Parallel(n_jobs=int(cpu_count()), prefer='processes')(\n",
    "        delayed(generate_random_ged_paris)(annotated_subgraph=annotated_subgraph, ged_dist=2, pairs_n=pairs_n, is_negative_example=True)\n",
    "        for annotated_subgraph in random_k_subgraphs\n",
    "    )\n",
    "\n",
    "with tqdm_joblib(tqdm(desc=\"My calculation\", total=total_tasks)) as progress_bar:\n",
    "    ged_1_examples = Parallel(n_jobs=int(cpu_count()), prefer='processes')(\n",
    "        delayed(generate_random_ged_paris)(annotated_subgraph=annotated_subgraph, ged_dist=1, pairs_n=pairs_n, is_negative_example=False)\n",
    "        for annotated_subgraph in random_k_subgraphs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# debug_ged_res = generate_random_ged_paris(annotated_subgraph=random_k_subgraphs[0], ged_dist=2, pairs_n=pairs_n, is_negative_example=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ged_examples1 = [elem for lst in ged_1_examples for elem in lst]\n",
    "ged_examples2 = [elem for lst in ged_2_examples for elem in lst]\n",
    "ged_examples = ged_examples1 + ged_examples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples_list = ged_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(ged_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for k_subgraph in k_subgraphs:\n",
    "    if k_subgraphs[0].nodes == k_subgraph.nodes:\n",
    "        print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(k_subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(train_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wwl\n",
    "from wwl import WL_metric\n",
    "from igraph import Graph\n",
    "from sklearn.metrics.pairwise import laplacian_kernel\n",
    "\n",
    "def get_graph_wl_distances(graphs, node_label):\n",
    "    igraphs = [Graph() for i in range(len(graphs))]\n",
    "    igraphs = [igraph.from_networkx(graph) for igraph, graph in zip(igraphs, graphs)]\n",
    "    kernel_values = wwl.pairwise_wasserstein_distance(igraphs, [len(igraphs)-1], enforce_continuous=True, num_iterations=10)\n",
    "    kernel_values = laplacian_kernel(kernel_values) #normalize energey\n",
    "\n",
    "    # gk_wl = WL_metric.GK_WL()\n",
    "    # kernel_values = gk_wl.compare_list(graphs, h=5, node_label=node_label)\n",
    "    return 1- kernel_values\n",
    "    #return np.round(1 - kernel_values, decimals=4)\n",
    "\n",
    "def generate_wl_dist_for_ged_pairs(ged_examples_list):\n",
    "    pairs_index_to_dist_map = {}\n",
    "\n",
    "    all_ged_example_list_graphs = []\n",
    "    for ged_example_list in ged_examples_list:\n",
    "        ged_example_list_graphs = [ged_example_list[0].subgraph.g] + [pair.masked_graph.g for pair in ged_example_list]\n",
    "        all_ged_example_list_graphs += ged_example_list_graphs\n",
    "\n",
    "    wl_dist_matrix = get_graph_wl_distances(all_ged_example_list_graphs, node_label=False)\n",
    "\n",
    "    base_graph_index = 0\n",
    "    pairs_index = 0\n",
    "    for ged_example_list_index, ged_example_list in enumerate(ged_examples_list):\n",
    "        for relative_index, pair in enumerate(ged_example_list):\n",
    "            wl_dist = wl_dist_matrix[base_graph_index][base_graph_index + relative_index + 1]\n",
    "            pairs_index_to_dist_map[pairs_index] = wl_dist\n",
    "\n",
    "            pairs_index += 1\n",
    "        base_graph_index += 1 + (relative_index + 1)\n",
    "\n",
    "    return pairs_index_to_dist_map\n",
    "\n",
    "# go over pairs in ged_examples1/2, and for their subgraphs generate wl+dist\n",
    "# only take into account their wl_dist from the all pairs vs all pairs matrix\n",
    "\n",
    "ged_1_examples_pairs_index_to_dist_map = {}\n",
    "ged_2_examples_pairs_index_to_dist_map = {}\n",
    "\n",
    "ged_examples_pairs_index_to_dist_map = generate_wl_dist_for_ged_pairs(ged_1_examples + ged_2_examples)\n",
    "for pair_index, wl_dist in ged_examples_pairs_index_to_dist_map.items():\n",
    "    if pair_index < len(ged_1_examples):\n",
    "        ged_1_examples_pairs_index_to_dist_map[pair_index] = wl_dist\n",
    "    else:\n",
    "        ged_2_examples_pairs_index_to_dist_map[pair_index] = wl_dist\n",
    "# ged_1_examples_pairs_index_to_dist_map = generate_wl_dist_for_ged_pairs(ged_1_examples)\n",
    "# ged_2_examples_pairs_index_to_dist_map = generate_wl_dist_for_ged_pairs(ged_2_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max(ged_1_examples_pairs_index_to_dist_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min(ged_2_examples_pairs_index_to_dist_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histogram(sequence, x_title=\"\"):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    sns.histplot(sequence, bins='auto', stat='density',\n",
    "                 label='Normalized Histogram', ax=ax)\n",
    "\n",
    "    # Fit a distribution to the data\n",
    "    mu, std = norm.fit(sequence)\n",
    "    x = np.linspace(min(sequence), max(sequence), 100)\n",
    "    if not np.all(x == x[0]):\n",
    "        y = norm.pdf(x, mu, std)\n",
    "    else:\n",
    "        y = np.ones_like(x)\n",
    "    sns.lineplot(x=x, y=y, color='red', label='Fitted Normal Distribution', ax=ax)\n",
    "\n",
    "    # Plot the KDE\n",
    "    sns.kdeplot(sequence, label='KDE', bw_adjust=0.5, ax=ax)\n",
    "\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{x_title} Distribution')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# set(ged_2_examples[0][1].masked_graph.g.edges).symmetric_difference(set(ged_2_examples[0][1].subgraph.g.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(list(ged_1_examples_pairs_index_to_dist_map.values()), \"GED1 WL distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_histogram(list(ged_2_examples_pairs_index_to_dist_map.values()), \"GED2 WL distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check 1-WL distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# optimized single call for WL distance calculation\n",
    "### NOTE: Wl dist is not stable (depends which batch of graph you applied it with) ###\n",
    "# wl_dist_matrix = get_graph_wl_distances(k_subgraphs, node_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# negative_pairs_wl_distances = []\n",
    "# positive_pairs_wl_distances = []\n",
    "#\n",
    "# positive_pairs_index_to_dist_map = {}\n",
    "# negative_pairs_index_to_dist_map = {}\n",
    "#\n",
    "# for i, train_sample in enumerate(train_samples_list):\n",
    "#     is_negative_sample = train_sample.is_negative_sample\n",
    "#     graph1 = train_sample.subgraph\n",
    "#     graph2 = train_sample.masked_graph\n",
    "#\n",
    "#     # s2v_graphs = generate_s2v_graphs([graph1.g, graph2.g], print_stats=False)\n",
    "#\n",
    "#     # distance = graph_metric_nn.forward([(s2v_graphs[0], s2v_graphs[1])]).item()\n",
    "#\n",
    "#     # wl_dist = get_graph_wl_distances([graph1.g, graph2.g], node_label=False)[0][1]\n",
    "#\n",
    "#     graph1_index, graph2_index = train_sample_indices_tuple_list[i]\n",
    "#     wl_dist = wl_dist_matrix[graph1_index][graph2_index]\n",
    "#     wl_dist = round(wl_dist, 6)\n",
    "#\n",
    "#     if is_negative_sample:\n",
    "#         negative_pairs_wl_distances.append(wl_dist)\n",
    "#         negative_pairs_index_to_dist_map[i] = wl_dist\n",
    "#     else:\n",
    "#         positive_pairs_wl_distances.append(wl_dist)\n",
    "#         positive_pairs_index_to_dist_map[i] = wl_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# len(positive_pairs_wl_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot_histogram(positive_pairs_wl_distances, \"positive WL distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot_histogram(negative_pairs_wl_distances, \"negative WL distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for i, dist in negative_pairs_index_to_dist_map.items():\n",
    "#     if dist < 0.13:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from subgraph_matching_via_nn.training.MarginLoss import MarginLoss\n",
    "# def calc_margin_loss(distances, pos_labels, margin):\n",
    "#     negative_labels = 1 - pos_labels\n",
    "#     lossCriterion = MarginLoss(margin)\n",
    "#     pos_loss, neg_loss = lossCriterion.get_loss(distances, pos_labels, negative_labels)\n",
    "#     print(pos_loss.sum().item())\n",
    "#     print(neg_loss.sum().item())\n",
    "#\n",
    "# margin = 0.001\n",
    "# calc_margin_loss(torch.tensor(positive_pairs_wl_distances+negative_pairs_wl_distances), torch.cat((torch.ones(len(positive_pairs_wl_distances)), torch.zeros(len(negative_pairs_wl_distances)))), margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit GNN on k subgraph pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# disable debugging overhead operations\n",
    "torch.autograd.set_detect_anomaly(mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "solver_params['margin_loss_margin_value'] = 0.25 #0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "used_train_samples_list = train_samples_list#[0:100]#[:10_000] #Needed due to performance issues\n",
    "processes_device_ids = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(used_train_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_samples_list = used_train_samples_list\n",
    "model_factory_func = lambda device: GraphCNN(num_layers=5, num_mlp_layers = 2, input_dim=input_dim, hidden_dim=128, output_dim=num_classes, final_dropout=0.5, learn_eps=False, graph_pooling_type=\"sum\", neighbor_pooling_type=\"sum\", device=device)\n",
    "solver_params[\"batch_size\"] = 512 * 32 #2048 #1024 #512\n",
    "solver_params[\"k_update_plot\"] = 1\n",
    "solver_params[\"lr\"] = 1e-4\n",
    "solver_params[\"max_epochs\"] = 2000\n",
    "trainer, graph_metric_nn, model = init_embedding_net_and_trainer(model_factory_func)\n",
    "\n",
    "\n",
    "# trainer.set_existing_data_loader_paths(\n",
    "#     ['./dataloaders/1702148493.0283618.p',\n",
    "#  './dataloaders/1702148497.9922142.p',\n",
    "#  './dataloaders/1702148502.9499629.p',\n",
    "#  './dataloaders/1702148511.1819935.p'],\n",
    "# []\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "trainer.train(processes_device_ids=processes_device_ids, use_existing_data_loaders=False, train_samples_list=used_train_samples_list, val_samples_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train again without rebuilding data loaders, and without the val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_factory_func = lambda device: GraphCNN(num_layers=5, num_mlp_layers = 2, input_dim=input_dim, hidden_dim=64, output_dim=num_classes, final_dropout=0.5, learn_eps=False, graph_pooling_type=\"sum\", neighbor_pooling_type=\"sum\", device=device)\n",
    "# new_trainer, graph_metric_nn, model = init_embedding_net_and_trainer(model_factory_func)\n",
    "\n",
    "# new_trainer.previous_train_loader_paths = trainer.previous_train_loader_paths\n",
    "# new_trainer.previous_val_loader_paths = [None for device_id in processes_device_ids] #trainer.previous_val_loader_paths\n",
    "# new_trainer.train(processes_device_ids=processes_device_ids, use_existing_data_loaders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# for i in range(0, len(train_samples_list)):\n",
    "#     is_negative_sample = train_samples_list[i].is_negative_sample\n",
    "#     graph1 = train_samples_list[i].subgraph\n",
    "#     graph2 = train_samples_list[i].masked_graph\n",
    "#\n",
    "#     s2v_graphs = generate_s2v_graphs([graph1.g, graph2.g], print_stats=False)\n",
    "#\n",
    "#     distance = graph_metric_nn.forward([(s2v_graphs[0], s2v_graphs[1])]).item()\n",
    "#\n",
    "#     if is_negative_sample and (distance < solver_params['margin_loss_margin_value']):\n",
    "#         print(f\"negative loss term not zero, as distance for this negative example pair is {distance}\")\n",
    "#     if (not is_negative_sample) and (distance > solver_params['margin_loss_margin_value']):\n",
    "#         print(f\"negative loss term not zero, as distance for this positive example pair is {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Execution time profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from cProfile import Profile\n",
    "# from pstats import SortKey, Stats\n",
    "\n",
    "# with Profile() as profile:\n",
    "#     new_trainer.train(processes_device_ids=processes_device_ids, use_existing_data_loaders=True)\n",
    "#     (\n",
    "#     Stats(profile)\n",
    "#         .strip_dirs()\n",
    "#         .sort_stats(SortKey.CUMULATIVE)\n",
    "#         .print_stats()\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
