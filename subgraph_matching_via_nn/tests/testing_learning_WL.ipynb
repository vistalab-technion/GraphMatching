{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "91d41c3ffd1af183",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-07T01:25:25.919338Z",
     "start_time": "2023-10-07T01:25:25.895805Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from itertools import combinations\n",
    "from livelossplot import PlotLosses\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def extract_subgraphs_and_indicators(graph, size_m, embedding_dim=16):\n",
    "    subgraph_embeddings = []\n",
    "    indicators = []\n",
    "    for node_set in combinations(graph.nodes(), size_m):\n",
    "        subgraph = graph.subgraph(node_set)\n",
    "        if len(subgraph.edges()) > 0:  # ensure there's at least one edge in the subgraph\n",
    "\n",
    "            # Compute the WL embedding for the subgraph\n",
    "            #wl_embedding = compute_WL_embedding(subgraph)\n",
    "            # TODO: replace by true WL embedding\n",
    "            wl_embedding = torch.rand(embedding_dim, 1)\n",
    "            subgraph_embeddings.append(wl_embedding)\n",
    "\n",
    "            # Create the indicator for nodes in the subgraph\n",
    "            indicator = [1 if node in node_set else 0 for node in graph.nodes()]\n",
    "            indicators.append(indicator)\n",
    "\n",
    "    return subgraph_embeddings, indicators\n",
    "\n",
    "\n",
    "n = 10\n",
    "m = 5\n",
    "embedding_dim = 5\n",
    "\n",
    "# Step 1: Construct a graph and generate its subgraphs\n",
    "G = nx.erdos_renyi_graph(n=n, p=0.5)\n",
    "\n",
    "# Extract all subgraphs of size m=10\n",
    "subgraph_embeddings, indicators = extract_subgraphs_and_indicators(G, m, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "# Create a dataset\n",
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, graph, indicators, subgraph_embeddings):\n",
    "        super(GraphDataset, self).__init__()\n",
    "        self.graph = graph\n",
    "        self.indicators = indicators\n",
    "        self.subgraph_embeddings = subgraph_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indicators)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert graph to Data object for PyTorch Geometric\n",
    "        edge_index = torch.tensor(list(self.graph.edges)).t().contiguous()\n",
    "\n",
    "        # Assign the indicator as the node feature\n",
    "        x = torch.tensor(self.indicators[idx], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        # The corresponding WL embedding for the subgraph\n",
    "        y = torch.tensor(self.subgraph_embeddings[idx], dtype=torch.float)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        return data\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "dataset = GraphDataset(G, indicators, subgraph_embeddings)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T01:25:25.923011Z",
     "start_time": "2023-10-07T01:25:25.910658Z"
    }
   },
   "id": "f3edb2f6c2e77138"
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, num_layers):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the first layer\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_channels, hidden_channels))\n",
    "\n",
    "        # Define the hidden layers\n",
    "        for _ in range(\n",
    "                num_layers - 2):  # -2 because we manually define the first and last layers\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        # Define the output layer\n",
    "        self.convs.append(GCNConv(hidden_channels, output_channels))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Apply the first layer\n",
    "        x = self.convs[0](x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply the hidden layers\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Apply the output layer\n",
    "        x = self.convs[self.num_layers - 1](x, edge_index)\n",
    "        x = global_mean_pool(x, data.batch)  # Pool to graph level\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "#model = GCN(input_channels=1, hidden_channels=16, output_channels=16, num_layers=3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T01:25:25.923139Z",
     "start_time": "2023-10-07T01:25:25.916189Z"
    }
   },
   "id": "f11320265ef1b85e"
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "output_dim = embedding_dim\n",
    "num_layers = 5\n",
    "kernel_size = n  # for pooling\n",
    "model = GCN(input_dim, hidden_dim, output_dim,\n",
    "            num_layers)  # 1 input feature, 32 hidden features, 16 output features (embedding size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T01:25:25.923617Z",
     "start_time": "2023-10-07T01:25:25.920007Z"
    }
   },
   "id": "64c03e6dfddce898"
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:05<00:13,  5.28it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train(model, epochs, train_loader, optimizer, criterion, k_plot):\n",
    "    liveloss = PlotLosses()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        logs = {}\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y.reshape(out.shape))\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        average_loss = running_loss / len(train_loader.dataset)\n",
    "        logs['loss'] = average_loss\n",
    "\n",
    "        if np.mod(epoch, k_plot):\n",
    "            liveloss.update(logs)\n",
    "            liveloss.send()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "train(model, epochs=100, train_loader=loader, optimizer=optimizer, criterion=criterion,\n",
    "      k_plot=10)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d88604998b788bca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming your dataset is defined as 'dataset'\n",
    "eval_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    for data in eval_loader:\n",
    "        out = model(data)\n",
    "        print(\"Predicted embedding:\", out)\n",
    "        print(\"Actual embedding:\", data.y.reshape(out.shape))\n",
    "        print(\"error:\", torch.norm(data.y.reshape(out.shape) - out))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-07T01:25:31.575085Z"
    }
   },
   "id": "618cc3b4131f2bb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Visualization and Evaluation\n",
    "\n",
    "# Extract all predicted and actual embeddings\n",
    "predicted_embeddings = []\n",
    "actual_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in loader:\n",
    "        out = model(data)\n",
    "        predicted_embeddings.extend(out.tolist())\n",
    "        actual_embeddings.extend(data.y.tolist())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predicted_embeddings = np.array(predicted_embeddings)\n",
    "actual_embeddings = np.array(actual_embeddings)\n",
    "\n",
    "# Compute the mean squared error between predicted and actual embeddings\n",
    "mse = mean_squared_error(actual_embeddings, predicted_embeddings)\n",
    "print(f\"Mean Squared Error between predicted and actual embeddings: {mse:.4f}\")\n",
    "\n",
    "# Use t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "all_embeddings = np.vstack((actual_embeddings, predicted_embeddings))\n",
    "embedded = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "# Split the t-SNE transformed embeddings\n",
    "actual_tsne = embedded[:len(actual_embeddings)]\n",
    "predicted_tsne = embedded[len(actual_embeddings):]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(actual_tsne[:, 0], actual_tsne[:, 1], c='r', label='Actual Embeddings')\n",
    "plt.scatter(predicted_tsne[:, 0], predicted_tsne[:, 1], c='b', marker='x',\n",
    "            label='Predicted Embeddings')\n",
    "plt.legend()\n",
    "plt.title(\"t-SNE visualization of embeddings\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-07T01:25:31.577152Z"
    }
   },
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
